{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports\n",
    "\n",
    "Run this cell first to get imports and build_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx\n",
    "import brevitas.onnx as bo\n",
    "\n",
    "from brevitas.core.quant import QuantType\n",
    "from finn.core.datatype import DataType\n",
    "from finn.core.modelwrapper import ModelWrapper\n",
    "from finn.custom_op.registry import getCustomOp\n",
    "from finn.util.basic import pynq_part_map\n",
    "from finn.util.pytorch import ToTensor\n",
    "from finn.util.visualization import showSrc, showInNetron\n",
    "from finn.transformation.infer_shapes import InferShapes\n",
    "from finn.transformation.infer_datatypes import InferDataTypes\n",
    "from finn.transformation.infer_data_layouts import InferDataLayouts\n",
    "from finn.transformation.general import GiveReadableTensorNames, GiveUniqueNodeNames, RemoveStaticGraphInputs\n",
    "from finn.transformation.general import RemoveUnusedTensors\n",
    "from finn.transformation.fold_constants import FoldConstants\n",
    "from finn.transformation.merge_onnx_models import MergeONNXModels\n",
    "from finn.transformation.streamline import Streamline\n",
    "from finn.transformation.streamline.round_thresholds import RoundAndClipThresholds\n",
    "from finn.transformation.streamline.reorder import MakeMaxPoolNHWC, MoveScalarLinearPastInvariants\n",
    "from finn.transformation.lower_convs_to_matmul import LowerConvsToMatMul\n",
    "from finn.transformation.insert_topk import InsertTopK\n",
    "from finn.transformation.fpgadataflow.create_dataflow_partition import CreateDataflowPartition\n",
    "from finn.transformation.fpgadataflow.make_zynq_proj import ZynqBuild\n",
    "from finn.transformation.fpgadataflow.make_deployment import DeployToPYNQ\n",
    "\n",
    "import finn.transformation.streamline.absorb as absorb\n",
    "import finn.transformation.streamline.reorder as reorder\n",
    "import finn.transformation.fpgadataflow.convert_to_hls_layers as to_hls\n",
    "\n",
    "\n",
    "build_dir = \"/workspace/finn/notebooks/fpga/mlp_v5\"\n",
    "\n",
    "model_name = build_dir + \"/mlp_feat_1111_1002.onnx\"\n",
    "model_tidy = build_dir + \"/mlp_tidy.onnx\"\n",
    "model_prepoc_chkpt = build_dir + \"/mlp_prepoc.onnx\"\n",
    "model_prepost = build_dir + \"/mlp_prepost.onnx\"\n",
    "model_ready_for_hls = build_dir + \"/mlp_ready_for_hls.onnx\"\n",
    "model_hls_layers = build_dir + \"/mlp_hls_layers.onnx\"\n",
    "model_dataflow_parent = build_dir + \"/mlp_dataflow_parent.onnx\"\n",
    "model_partition = build_dir + \"/mlp_partition.onnx\"\n",
    "model_set_folding_factors = build_dir + \"/mlp_set_folding_factors.onnx\"\n",
    "model_partition = build_dir + \"/mlp_partition.onnx\"\n",
    "model_post_synthesis = build_dir + \"/mlp_post_synthesis.onnx\"\n",
    "model_pynq_deploy = build_dir + \"/mlp_pynq_deploy.onnx\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tidy the onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ModelWrapper(model_name)\n",
    "model = model.transform(InferShapes())\n",
    "model = model.transform(GiveUniqueNodeNames())\n",
    "model = model.transform(GiveReadableTensorNames())\n",
    "model = model.transform(InferDataTypes())\n",
    "model = model.transform(RemoveStaticGraphInputs())\n",
    "model.save(model_tidy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "showInNetron(model_tidy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing and postprocessing\n",
    "\n",
    "Preprocess input by exporting a single-node ONNX graph for division by 255 (which already exists as finn.util.pytorch.ToTensor and merging this with our original model\n",
    "Then, mark the input tensor as 8-bit to let FINN know which level of precision to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ModelWrapper(model_tidy)\n",
    "global_inp_name = model.graph.input[0].name\n",
    "ishape = model.get_tensor_shape(global_inp_name)\n",
    "print(ishape)\n",
    "\n",
    "bo.export_finn_onnx(ToTensor(), ishape, model_prepoc_chkpt)\n",
    "\n",
    "# join preprocessing and core model\n",
    "pre_model = ModelWrapper(model_prepoc_chkpt)\n",
    "model = model.transform(MergeONNXModels(pre_model))\n",
    "\n",
    "# add input quantization annotation: UINT8 for BNN (finn default)\n",
    "global_inp_name = model.graph.input[0].name\n",
    "model.set_tensor_datatype(global_inp_name, DataType.UINT8)\n",
    "\n",
    "model.save(model_prepost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "showInNetron(model_prepost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Streamlining\n",
    "Use in-built FINN functions to do streamlining (eliminate floating point operations by moving them around, then collapsing them into one operation and in the last step transform them into multi-thresholding nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model = ModelWrapper(model_prepost)\n",
    "\n",
    "model = model.transform(InsertTopK(k=1))\n",
    "model = model.transform(MoveScalarLinearPastInvariants())\n",
    "model = model.transform(Streamline())\n",
    "model = model.transform(InferDataLayouts())\n",
    "model = model.transform(RemoveUnusedTensors())\n",
    "\n",
    "model = model.transform(reorder.MoveScalarLinearPastInvariants())\n",
    "model = model.transform(reorder.MoveScalarAddPastMatMul())\n",
    "model = model.transform(reorder.MoveScalarMulPastMatMul())\n",
    "\n",
    "model = model.transform(absorb.AbsorbAddIntoMultiThreshold())\n",
    "model = model.transform(absorb.AbsorbMulIntoMultiThreshold())\n",
    "model = model.transform(absorb.AbsorbScalarMulAddIntoTopK())\n",
    "model = model.transform(RoundAndClipThresholds())\n",
    "model = model.transform(InferDataLayouts())\n",
    "model = model.transform(RemoveUnusedTensors())\n",
    "\n",
    "model.save(model_ready_for_hls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "showInNetron(model_ready_for_hls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = ModelWrapper(model_ready_for_hls)\n",
    "model = model.transform(to_hls.InferAddStreamsLayer())\n",
    "model = model.transform(to_hls.InferThresholdingLayer())\n",
    "model = model.transform(to_hls.InferQuantizedStreamingFCLayer(\"decoupled\"))\n",
    "model = model.transform(to_hls.InferChannelwiseLinearLayer())\n",
    "model.save(model_hls_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "showInNetron(model_hls_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = ModelWrapper(model_hls_layers)\n",
    "parent_model = model.transform(CreateDataflowPartition())\n",
    "parent_model.save(model_dataflow_parent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "showInNetron(model_dataflow_parent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sdp_node = parent_model.get_nodes_by_op_type(\"StreamingDataflowPartition\")[0]\n",
    "sdp_node = getCustomOp(sdp_node)\n",
    "dataflow_model_filename = sdp_node.get_nodeattr(\"model\")\n",
    "model = ModelWrapper(dataflow_model_filename)\n",
    "model.save(model_partition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "showInNetron(model_partition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fc0 = model.graph.node[0]\n",
    "fc0w = getCustomOp(fc0)\n",
    "\n",
    "#print(\"CustomOp wrapper is of class \" + fc0w.__class__.__name__)\n",
    "#fc0w.get_nodeattr_types()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fc_layers = model.get_nodes_by_op_type(\"StreamingFCLayer_Batch\")\n",
    "# (PE, SIMD, in_fifo_depth, out_fifo_depth, ramstyle) for each layer\n",
    "\n",
    "config = [\n",
    "    (16, 49, 16, 64, \"auto\"),\n",
    "    (10, 8, 64, 10, \"auto\"),\n",
    "    (8, 8, 64, 64, \"auto\"),\n",
    "]\n",
    "\n",
    "\n",
    "for fcl, (pe, simd, ififo, ofifo, ramstyle) in zip(fc_layers, config):\n",
    "    fcl_inst = getCustomOp(fcl)\n",
    "    fcl_inst.set_nodeattr(\"PE\", pe)\n",
    "    fcl_inst.set_nodeattr(\"SIMD\", simd)\n",
    "    fcl_inst.set_nodeattr(\"inFIFODepth\", ififo)\n",
    "    fcl_inst.set_nodeattr(\"outFIFODepth\", ofifo)\n",
    "    fcl_inst.set_nodeattr(\"ram_style\", ramstyle)\n",
    "    \n",
    "# set parallelism for input quantizer to be same as first layer's SIMD\n",
    "inp_qnt_node = model.get_nodes_by_op_type(\"Thresholding_Batch\")[0]\n",
    "inp_qnt = getCustomOp(inp_qnt_node)\n",
    "inp_qnt.set_nodeattr(\"PE\", 49)\n",
    "model.save(model_set_folding_factors)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "showInNetron(model_set_folding_factors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hardware build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pynq_board = \"Ultra96\"\n",
    "fpga_part = pynq_part_map[pynq_board]\n",
    "target_clk_ns = 10\n",
    "#target_clk_ns = 5\n",
    "#target_clk_ns = 20\n",
    "model = ModelWrapper(model_partition)\n",
    "model = model.transform(ZynqBuild(platform = pynq_board, period_ns = target_clk_ns))\n",
    "model.save(model_post_synthesis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "showInNetron(model_post_synthesis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = ModelWrapper(model_post_synthesis)\n",
    "sdp_node_middle = getCustomOp(model.graph.node[1])\n",
    "postsynth_layers = sdp_node_middle.get_nodeattr(\"model\")\n",
    "\n",
    "showInNetron(postsynth_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ModelWrapper(postsynth_layers)\n",
    "model.model.metadata_props"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil  \n",
    "print (model.get_metadata_prop(\"pynq_driver_dir\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deployment\n",
    "\n",
    "We'll now use the `DeployToPYNQ` transformation to create a deployment folder with the bitfile and driver file(s), and copy that to the PYNQ board (can let it run for a while then stop to get the bitfiles, no need to wait for the whole cell to run finish, since it can't ssh into the Ultra96)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ip = \"192.168.2.99\"\n",
    "port = \"22\"\n",
    "username = \"xilinx\"\n",
    "password = \"xilinx\"\n",
    "target_dir = \"/home/xilinx/finn_dev_wkexin\"\n",
    "model = model.transform(DeployToPYNQ(ip, port, username, password, target_dir))\n",
    "model.save(build_dir + \"/mlp_pynq_deploy.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "showInNetron(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
